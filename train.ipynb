{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids, counts=None):\n",
    "    \"\"\"\n",
    "    Given a list of integers, return a dictionary of counts of consecutive pairs\n",
    "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
    "    Optionally allows to update an existing dictionary of counts\n",
    "    \"\"\"\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids, ids[1:]): # iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    \"\"\"\n",
    "    In the list of integers (ids), replace all consecutive occurrences\n",
    "    of pair with the new integer token idx\n",
    "    Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
    "    \"\"\"\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if not at the very last position AND the pair matches, replace it\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "\n",
    "\n",
    "# the main GPT text split patterns, see\n",
    "# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "pattern = GPT4_SPLIT_PATTERN \n",
    "compiled_pattern = re.compile(pattern)\n",
    "special_tokens = {}\n",
    "inverse_special_tokens = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merges = {}\n",
    "def train(text, vocab_size, verbose=True):\n",
    "    assert vocab_size >= 256\n",
    "    num_merges = vocab_size - 256\n",
    "\n",
    "    # split the text up into text chunks\n",
    "    text_chunks = re.findall(compiled_pattern, text)\n",
    "\n",
    "    # input text preprocessing\n",
    "    ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n",
    "\n",
    "    # iteratively merge the most common pairs to create new tokens\n",
    "    merges = {} # (int, int) -> int\n",
    "    vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes\n",
    "    for i in range(num_merges):\n",
    "        # count the number of times every consecutive pair appears\n",
    "        stats = {}\n",
    "        for chunk_ids in ids:\n",
    "            # passing in stats will update it in place, adding up counts\n",
    "            get_stats(chunk_ids, stats)\n",
    "        # find the pair with the highest count\n",
    "        pair = max(stats, key=stats.get)\n",
    "        # mint a new token: assign it the next available id\n",
    "        idx = 256 + i\n",
    "        # replace all occurrences of pair in ids with idx\n",
    "        ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]\n",
    "        # save the merge\n",
    "        merges[pair] = idx\n",
    "        vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "        # prints\n",
    "        if verbose:\n",
    "            print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_vocab():\n",
    "    # vocab is simply and deterministically derived from merges\n",
    "    vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "    for (p0, p1), idx in merges.items():\n",
    "        vocab[idx] = vocab[p0] + vocab[p1]\n",
    "    for special, idx in special_tokens.items():\n",
    "        vocab[idx] = special.encode(\"utf-8\")\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = _build_vocab()\n",
    "\n",
    "\n",
    "def register_special_tokens(special_tokens):\n",
    "    # special_tokens is a dictionary of str -> int\n",
    "    # example: {\"<|endoftext|>\": 100257}\n",
    "    special_tokens = special_tokens\n",
    "    inverse_special_tokens = {v: k for k, v in special_tokens.items()}\n",
    "\n",
    "def decode(ids):\n",
    "    # given ids (list of integers), return Python string\n",
    "    part_bytes = []\n",
    "    for idx in ids:\n",
    "        if idx in vocab:\n",
    "            part_bytes.append(vocab[idx])\n",
    "        elif idx in inverse_special_tokens:\n",
    "            part_bytes.append(inverse_special_tokens[idx].encode(\"utf-8\"))\n",
    "        else:\n",
    "            raise ValueError(f\"invalid token id: {idx}\")\n",
    "    text_bytes = b\"\".join(part_bytes)\n",
    "    text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text\n",
    "\n",
    "def _encode_chunk(text_bytes):\n",
    "    # return the token ids\n",
    "    # let's begin. first, convert all bytes to integers in range 0..255\n",
    "    ids = list(text_bytes)\n",
    "    while len(ids) >= 2:\n",
    "        # find the pair with the lowest merge index\n",
    "        stats = get_stats(ids)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        # subtle: if there are no more merges available, the key will\n",
    "        # result in an inf for every single pair, and the min will be\n",
    "        # just the first pair in the list, arbitrarily\n",
    "        # we can detect this terminating case by a membership check\n",
    "        if pair not in merges:\n",
    "            break # nothing else can be merged anymore\n",
    "        # otherwise let's merge the best pair (lowest merge index)\n",
    "        idx = merges[pair]\n",
    "        ids = merge(ids, pair, idx)\n",
    "    return ids\n",
    "\n",
    "def encode_ordinary(text):\n",
    "    \"\"\"Encoding that ignores any special tokens.\"\"\"\n",
    "    # split text into chunks of text by categories defined in regex pattern\n",
    "    text_chunks = re.findall(compiled_pattern, text)\n",
    "    # all chunks of text are encoded separately, then results are joined\n",
    "    ids = []\n",
    "    for chunk in text_chunks:\n",
    "        chunk_bytes = chunk.encode(\"utf-8\") # raw bytes\n",
    "        chunk_ids = _encode_chunk(chunk_bytes)\n",
    "        ids.extend(chunk_ids)\n",
    "    return ids\n",
    "\n",
    "def encode(text, allowed_special=\"none_raise\"):\n",
    "    \"\"\"\n",
    "    Unlike encode_ordinary, this function handles special tokens.\n",
    "    allowed_special: can be \"all\"|\"none\"|\"none_raise\" or a custom set of special tokens\n",
    "    if none_raise, then an error is raised if any special token is encountered in text\n",
    "    this is the default tiktoken behavior right now as well\n",
    "    any other behavior is either annoying, or a major footgun\n",
    "    \"\"\"\n",
    "    # decode the user desire w.r.t. handling of special tokens\n",
    "    special = None\n",
    "    if allowed_special == \"all\":\n",
    "        special = special_tokens\n",
    "    elif allowed_special == \"none\":\n",
    "        special = {}\n",
    "    elif allowed_special == \"none_raise\":\n",
    "        special = {}\n",
    "        assert all(token not in text for token in special_tokens)\n",
    "    elif isinstance(allowed_special, set):\n",
    "        special = {k: v for k, v in special_tokens.items() if k in allowed_special}\n",
    "    else:\n",
    "        raise ValueError(f\"allowed_special={allowed_special} not understood\")\n",
    "    if not special:\n",
    "        # shortcut: if no special tokens, just use the ordinary encoding\n",
    "        return encode_ordinary(text)\n",
    "    # otherwise, we have to be careful with potential special tokens in text\n",
    "    # we handle special tokens by splitting the text\n",
    "    # based on the occurrence of any exact match with any of the special tokens\n",
    "    # we can use re.split for this. note that surrounding the pattern with ()\n",
    "    # makes it into a capturing group, so the special tokens will be included\n",
    "    special_pattern = \"(\" + \"|\".join(re.escape(k) for k in special) + \")\"\n",
    "    special_chunks = re.split(special_pattern, text)\n",
    "    # now all the special characters are separated from the rest of the text\n",
    "    # all chunks of text are encoded separately, then results are joined\n",
    "    ids = []\n",
    "\n",
    "    for part in special_chunks:\n",
    "        if part in special:\n",
    "            # this is a special token, encode it separately as a special case\n",
    "            ids.append(special[part])\n",
    "        else:\n",
    "            # this is an ordinary sequence, encode it normally\n",
    "            ids.extend(encode_ordinary(part))\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge 1/44: (105, 110) -> 256 (b'in') had 16 occurrences\n",
      "merge 2/44: (32, 116) -> 257 (b' t') had 15 occurrences\n",
      "merge 3/44: (32, 97) -> 258 (b' a') had 14 occurrences\n",
      "merge 4/44: (114, 101) -> 259 (b're') had 11 occurrences\n",
      "merge 5/44: (101, 110) -> 260 (b'en') had 10 occurrences\n",
      "merge 6/44: (101, 114) -> 261 (b'er') had 10 occurrences\n",
      "merge 7/44: (97, 116) -> 262 (b'at') had 9 occurrences\n",
      "merge 8/44: (111, 110) -> 263 (b'on') had 9 occurrences\n",
      "merge 9/44: (100, 101) -> 264 (b'de') had 9 occurrences\n",
      "merge 10/44: (111, 114) -> 265 (b'or') had 8 occurrences\n",
      "merge 11/44: (32, 256) -> 266 (b' in') had 8 occurrences\n",
      "merge 12/44: (32, 103) -> 267 (b' g') had 7 occurrences\n",
      "merge 13/44: (262, 105) -> 268 (b'ati') had 7 occurrences\n",
      "merge 14/44: (111, 264) -> 269 (b'ode') had 7 occurrences\n",
      "merge 15/44: (114, 111) -> 270 (b'ro') had 7 occurrences\n",
      "merge 16/44: (32, 109) -> 271 (b' m') had 6 occurrences\n",
      "merge 17/44: (269, 108) -> 272 (b'odel') had 6 occurrences\n",
      "merge 18/44: (257, 111) -> 273 (b' to') had 6 occurrences\n",
      "merge 19/44: (71, 80) -> 274 (b'GP') had 6 occurrences\n",
      "merge 20/44: (274, 84) -> 275 (b'GPT') had 6 occurrences\n",
      "merge 21/44: (110, 100) -> 276 (b'nd') had 6 occurrences\n",
      "merge 22/44: (32, 99) -> 277 (b' c') had 6 occurrences\n",
      "merge 23/44: (268, 263) -> 278 (b'ation') had 5 occurrences\n",
      "merge 24/44: (272, 115) -> 279 (b'odels') had 5 occurrences\n",
      "merge 25/44: (114, 97) -> 280 (b'ra') had 5 occurrences\n",
      "merge 26/44: (32, 108) -> 281 (b' l') had 5 occurrences\n",
      "merge 27/44: (32, 275) -> 282 (b' GPT') had 5 occurrences\n",
      "merge 28/44: (32, 111) -> 283 (b' o') had 5 occurrences\n",
      "merge 29/44: (101, 115) -> 284 (b'es') had 5 occurrences\n",
      "merge 30/44: (112, 270) -> 285 (b'pro') had 5 occurrences\n",
      "merge 31/44: (109, 112) -> 286 (b'mp') had 5 occurrences\n",
      "merge 32/44: (256, 103) -> 287 (b'ing') had 5 occurrences\n",
      "merge 33/44: (101, 120) -> 288 (b'ex') had 4 occurrences\n",
      "merge 34/44: (267, 260) -> 289 (b' gen') had 4 occurrences\n",
      "merge 35/44: (289, 261) -> 290 (b' gener') had 4 occurrences\n",
      "merge 36/44: (32, 115) -> 291 (b' s') had 4 occurrences\n",
      "merge 37/44: (281, 105) -> 292 (b' li') had 4 occurrences\n",
      "merge 38/44: (292, 107) -> 293 (b' lik') had 4 occurrences\n",
      "merge 39/44: (293, 101) -> 294 (b' like') had 4 occurrences\n",
      "merge 40/44: (258, 276) -> 295 (b' and') had 4 occurrences\n",
      "merge 41/44: (108, 108) -> 296 (b'll') had 4 occurrences\n",
      "merge 42/44: (117, 116) -> 297 (b'ut') had 4 occurrences\n",
      "merge 43/44: (257, 288) -> 298 (b' tex') had 3 occurrences\n",
      "merge 44/44: (298, 116) -> 299 (b' text') had 3 occurrences\n"
     ]
    }
   ],
   "source": [
    "train(\"\"\"OpenAI's text generation models (often referred to as generative pre-trained transformers or \"GPT\" models for short), like GPT-4 and GPT-3.5, have been trained to understand natural and formal language. Models like GPT-4 allows text outputs in response to their inputs. The inputs to these models are also referred to as \"prompts\". Designing a prompt is essentially how you \"program\" a model like GPT-4, usually by providing instructions or some examples of how to successfully complete a task. Models like GPT-4 can be used across a great variety of tasks including content or code generation, summarization, conversation, creative writing, and more. Read more in our introductory text generation guide and in our prompt engineering guide.\"\"\", vocab_size=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of class Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal (byte-level) Byte Pair Encoding tokenizer.\n",
    "\n",
    "Algorithmically follows along the GPT tokenizer:\n",
    "https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
    "\n",
    "Unlike BasicTokenizer:\n",
    "- RegexTokenizer handles an optional regex splitting pattern.\n",
    "- RegexTokenizer handles optional special tokens.\n",
    "\"\"\"\n",
    "\n",
    "import regex as re\n",
    "\n",
    "\n",
    "\n",
    "# the main GPT text split patterns, see\n",
    "# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "\n",
    "class RegexTokenizer():\n",
    "\n",
    "    def __init__(self, pattern=None):\n",
    "        \"\"\"\n",
    "        - pattern: optional string to override the default (GPT-4 split pattern)\n",
    "        - special_tokens: str -> int dictionary of special tokens\n",
    "          example: {'<|endoftext|>': 100257}\n",
    "        \"\"\"\n",
    "\n",
    "        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n",
    "        self.compiled_pattern = re.compile(self.pattern)\n",
    "        self.special_tokens = {}\n",
    "        self.inverse_special_tokens = {}\n",
    "        self.merges = {}\n",
    "        self.vocab = self._build_vocab()\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=True):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "\n",
    "        # split the text up into text chunks\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "\n",
    "        # input text preprocessing\n",
    "        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n",
    "\n",
    "        # iteratively merge the most common pairs to create new tokens\n",
    "        merges = {} # (int, int) -> int\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes\n",
    "        for i in range(num_merges):\n",
    "            # count the number of times every consecutive pair appears\n",
    "            stats = {}\n",
    "            for chunk_ids in ids:\n",
    "                # passing in stats will update it in place, adding up counts\n",
    "                get_stats(chunk_ids, stats)\n",
    "            # find the pair with the highest count\n",
    "            pair = max(stats, key=stats.get)\n",
    "            # mint a new token: assign it the next available id\n",
    "            idx = 256 + i\n",
    "            # replace all occurrences of pair in ids with idx\n",
    "            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]\n",
    "            # save the merge\n",
    "            merges[pair] = idx\n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "            # prints\n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n",
    "\n",
    "        # save class variables\n",
    "        self.merges = merges # used in encode()\n",
    "        self.vocab = vocab   # used in decode()\n",
    "\n",
    "    def register_special_tokens(self, special_tokens):\n",
    "        # special_tokens is a dictionary of str -> int\n",
    "        # example: {\"<|endoftext|>\": 100257}\n",
    "        self.special_tokens = special_tokens\n",
    "        self.inverse_special_tokens = {v: k for k, v in special_tokens.items()}\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # given ids (list of integers), return Python string\n",
    "        part_bytes = []\n",
    "        for idx in ids:\n",
    "            if idx in self.vocab:\n",
    "                part_bytes.append(self.vocab[idx])\n",
    "            elif idx in self.inverse_special_tokens:\n",
    "                part_bytes.append(self.inverse_special_tokens[idx].encode(\"utf-8\"))\n",
    "            else:\n",
    "                raise ValueError(f\"invalid token id: {idx}\")\n",
    "        text_bytes = b\"\".join(part_bytes)\n",
    "        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "    \n",
    "    def _build_vocab(self):\n",
    "        # vocab is simply and deterministically derived from merges\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        for special, idx in self.special_tokens.items():\n",
    "            vocab[idx] = special.encode(\"utf-8\")\n",
    "        return vocab\n",
    "\n",
    "    def _encode_chunk(self, text_bytes):\n",
    "        # return the token ids\n",
    "        # let's begin. first, convert all bytes to integers in range 0..255\n",
    "        ids = list(text_bytes)\n",
    "        while len(ids) >= 2:\n",
    "            # find the pair with the lowest merge index\n",
    "            stats = get_stats(ids)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            # subtle: if there are no more merges available, the key will\n",
    "            # result in an inf for every single pair, and the min will be\n",
    "            # just the first pair in the list, arbitrarily\n",
    "            # we can detect this terminating case by a membership check\n",
    "            if pair not in self.merges:\n",
    "                break # nothing else can be merged anymore\n",
    "            # otherwise let's merge the best pair (lowest merge index)\n",
    "            idx = self.merges[pair]\n",
    "            ids = merge(ids, pair, idx)\n",
    "        return ids\n",
    "\n",
    "    def encode_ordinary(self, text):\n",
    "        \"\"\"Encoding that ignores any special tokens.\"\"\"\n",
    "        # split text into chunks of text by categories defined in regex pattern\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "        # all chunks of text are encoded separately, then results are joined\n",
    "        ids = []\n",
    "        for chunk in text_chunks:\n",
    "            chunk_bytes = chunk.encode(\"utf-8\") # raw bytes\n",
    "            chunk_ids = self._encode_chunk(chunk_bytes)\n",
    "            ids.extend(chunk_ids)\n",
    "        return ids\n",
    "\n",
    "    def encode(self, text, allowed_special=\"none_raise\"):\n",
    "        \"\"\"\n",
    "        Unlike encode_ordinary, this function handles special tokens.\n",
    "        allowed_special: can be \"all\"|\"none\"|\"none_raise\" or a custom set of special tokens\n",
    "        if none_raise, then an error is raised if any special token is encountered in text\n",
    "        this is the default tiktoken behavior right now as well\n",
    "        any other behavior is either annoying, or a major footgun\n",
    "        \"\"\"\n",
    "        # decode the user desire w.r.t. handling of special tokens\n",
    "        special = None\n",
    "        if allowed_special == \"all\":\n",
    "            special = self.special_tokens\n",
    "        elif allowed_special == \"none\":\n",
    "            special = {}\n",
    "        elif allowed_special == \"none_raise\":\n",
    "            special = {}\n",
    "            assert all(token not in text for token in self.special_tokens)\n",
    "        elif isinstance(allowed_special, set):\n",
    "            special = {k: v for k, v in self.special_tokens.items() if k in allowed_special}\n",
    "        else:\n",
    "            raise ValueError(f\"allowed_special={allowed_special} not understood\")\n",
    "        if not special:\n",
    "            # shortcut: if no special tokens, just use the ordinary encoding\n",
    "            return self.encode_ordinary(text)\n",
    "        # otherwise, we have to be careful with potential special tokens in text\n",
    "        # we handle special tokens by splitting the text\n",
    "        # based on the occurrence of any exact match with any of the special tokens\n",
    "        # we can use re.split for this. note that surrounding the pattern with ()\n",
    "        # makes it into a capturing group, so the special tokens will be included\n",
    "        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in special) + \")\"\n",
    "        special_chunks = re.split(special_pattern, text)\n",
    "        # now all the special characters are separated from the rest of the text\n",
    "        # all chunks of text are encoded separately, then results are joined\n",
    "        ids = []\n",
    "\n",
    "        for part in special_chunks:\n",
    "            if part in special:\n",
    "                # this is a special token, encode it separately as a special case\n",
    "                ids.append(special[part])\n",
    "            else:\n",
    "                # this is an ordinary sequence, encode it normally\n",
    "                ids.extend(self.encode_ordinary(part))\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = RegexTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge 1/44: (121, 111) -> 256 (b'yo') had 8 occurrences\n",
      "merge 2/44: (101, 114) -> 257 (b'er') had 8 occurrences\n",
      "merge 3/44: (32, 97) -> 258 (b' a') had 7 occurrences\n",
      "merge 4/44: (32, 256) -> 259 (b' yo') had 7 occurrences\n",
      "merge 5/44: (259, 117) -> 260 (b' you') had 7 occurrences\n",
      "merge 6/44: (101, 118) -> 261 (b'ev') had 7 occurrences\n",
      "merge 7/44: (32, 116) -> 262 (b' t') had 7 occurrences\n",
      "merge 8/44: (32, 115) -> 263 (b' s') had 6 occurrences\n",
      "merge 9/44: (105, 116) -> 264 (b'it') had 6 occurrences\n",
      "merge 10/44: (32, 112) -> 265 (b' p') had 5 occurrences\n",
      "merge 11/44: (114, 101) -> 266 (b're') had 5 occurrences\n",
      "merge 12/44: (110, 101) -> 267 (b'ne') had 5 occurrences\n",
      "merge 13/44: (32, 119) -> 268 (b' w') had 5 occurrences\n",
      "merge 14/44: (101, 110) -> 269 (b'en') had 5 occurrences\n",
      "merge 15/44: (104, 101) -> 270 (b'he') had 4 occurrences\n",
      "merge 16/44: (32, 105) -> 271 (b' i') had 4 occurrences\n",
      "merge 17/44: (32, 102) -> 272 (b' f') had 4 occurrences\n",
      "merge 18/44: (118, 105) -> 273 (b'vi') had 4 occurrences\n",
      "merge 19/44: (265, 114) -> 274 (b' pr') had 4 occurrences\n",
      "merge 20/44: (10, 10) -> 275 (b'\\n\\n') had 4 occurrences\n",
      "merge 21/44: (108, 108) -> 276 (b'll') had 3 occurrences\n",
      "merge 22/44: (116, 97) -> 277 (b'ta') had 3 occurrences\n",
      "merge 23/44: (32, 104) -> 278 (b' h') had 3 occurrences\n",
      "merge 24/44: (111, 114) -> 279 (b'or') had 3 occurrences\n",
      "merge 25/44: (116, 257) -> 280 (b'ter') had 3 occurrences\n",
      "merge 26/44: (263, 101) -> 281 (b' se') had 3 occurrences\n",
      "merge 27/44: (32, 261) -> 282 (b' ev') had 3 occurrences\n",
      "merge 28/44: (115, 101) -> 283 (b'se') had 3 occurrences\n",
      "merge 29/44: (262, 270) -> 284 (b' the') had 3 occurrences\n",
      "merge 30/44: (32, 100) -> 285 (b' d') had 3 occurrences\n",
      "merge 31/44: (268, 264) -> 286 (b' wit') had 3 occurrences\n",
      "merge 32/44: (286, 104) -> 287 (b' with') had 3 occurrences\n",
      "merge 33/44: (46, 275) -> 288 (b'.\\n\\n') had 3 occurrences\n",
      "merge 34/44: (110, 103) -> 289 (b'ng') had 3 occurrences\n",
      "merge 35/44: (97, 115) -> 290 (b'as') had 2 occurrences\n",
      "merge 36/44: (278, 111) -> 291 (b' ho') had 2 occurrences\n",
      "merge 37/44: (258, 266) -> 292 (b' are') had 2 occurrences\n",
      "merge 38/44: (32, 73) -> 293 (b' I') had 2 occurrences\n",
      "merge 39/44: (293, 110) -> 294 (b' In') had 2 occurrences\n",
      "merge 40/44: (294, 280) -> 295 (b' Inter') had 2 occurrences\n",
      "merge 41/44: (295, 267) -> 296 (b' Interne') had 2 occurrences\n",
      "merge 42/44: (296, 116) -> 297 (b' Internet') had 2 occurrences\n",
      "merge 43/44: (99, 101) -> 298 (b'ce') had 2 occurrences\n",
      "merge 44/44: (274, 111) -> 299 (b' pro') had 2 occurrences\n"
     ]
    }
   ],
   "source": [
    "tk.train(text=\"\"\" \"\"\", vocab_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 270\n",
      "ids [104, 101, 108, 108, 111]\n",
      "new ids [270, 108, 108, 111]\n",
      "idx 276\n",
      "ids [270, 108, 108, 111]\n",
      "new ids [270, 276, 111]\n",
      "idx 258\n",
      "ids [32, 97, 117]\n",
      "new ids [258, 117]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[270, 276, 111, 258, 117, 63, 63]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.encode('hello au??')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.decode([32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(97, 117): 256, (256, 116): 257}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'au',\n",
       " 257: b'aut'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
